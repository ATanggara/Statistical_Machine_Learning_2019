{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will practice minimising a cost function with gradient descent.\n",
    "\n",
    "### Assumed knowledge\n",
    "- Linear algebra (see Sam Roweis' notes, linked below, for matrix calculus tips)\n",
    "- Python programming\n",
    "- Preferably: Using numpy for matrix calculations (precourse material)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Using numpy ndarrays for matrix calculations\n",
    "- Using scipy.optimise routines to minimise a cost function, with and without a gradient\n",
    "- Randomly generating input values for testing\n",
    "\n",
    "## Pre-lab notes\n",
    "In this lab, you will apply linear algebra to to minimise a cost function in three steps: implementing the cost function, implementing a gradient function, and applying gradient descent. We will be doing this to solve problems throughout the course.\n",
    "\n",
    "As in all labs, feel free to skip questions if you get stuck, and ask your tutor if you have any questions!\n",
    "\n",
    "A note on style: in this course we emphasise *functional decomposition* in code style. Avoid using global variables, and remember that often splitting code off into separate functions can make it more readable and testable. (Jupyter notebooks let you call functions defined in previous cells.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up python environment (this cell contains Latex macros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as opt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *cost function* or *loss function* is the function we want to minimize in a given problem. For example, it might measure the error between the indicator values our models predicts and the true values for the training data. In this lab, we consider a toy example. We will define a cost function $f(X)$ where $X$ is a $n\\times p$ matrix.\n",
    "\n",
    "If $A$ is a square matrix, then we write $\\trace{A}$ for its trace. Let $ \\Norm{A}_F = \\sqrt{\\trace{A^T A}} $, the *Frobenius norm* of a matrix.\n",
    "\n",
    "Let our cost function $f(X)$ be defined for $n\\times p$ matrices $X$ as follows. Let $C$ be a fixed symmetric $n\\times n$ matrix (so $C = C^T$). Let $\\mu$ be a scalar that is larger than the $p^{th}$ smallest eigenvalue of $C$. Let $N$ be a diagonal $p\\times p$ matrix with distinct positive entries on the diagonal.\n",
    "\n",
    "The cost function is defined as\n",
    "\\begin{equation}\n",
    "  f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F\n",
    "\\end{equation}\n",
    "where $ X \\in \\RR^{n \\times p} $, $ n \\ge p $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenious Norm\n",
    "\n",
    "Implement a Python function ```frobenius_norm``` which accepts an arbitrary matrix $ A $ and returns\n",
    "$ \\Norm{A}_F $ using the formula given. (Use ```numpy.trace``` and ```numpy.sqrt```.) We represent matrices and vectors as numpy ndarrays.\n",
    "1. Given a matrix $ A \\in \\RR^{n \\times p} $, what is the complexity of your implementation of ```frobenius_norm```\n",
    "using the formula above?\n",
    "2. Can you come up with a faster implementation, if you were additionally told that $ p \\ge n $ ?\n",
    "\n",
    "Extension: Can you find an even faster implementation than in 1. and 2.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "\n",
    "1. Matrix transpose complexity = $O(n \\times p)$  \n",
    "   Matrix multiplication complexity = $O(n^2 \\times p)$  \n",
    "   Trace complexity = $O(n)$  \n",
    "   Total complexity = $O(n^2 \\times p + n \\times p + n) \\approx O(n^2 \\times p) $\n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "def frobenius_norm(A):\n",
    "    return np.sqrt(np.trace(A.T@A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function, ```cost_function_for_matrix```, which implements the function $f(X)$ defined above.\n",
    "\n",
    "Hint: What should the arguments to this function be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "def cost_function_for_matrix(X, C, N, mu):\n",
    "    return (1/2)*np.trace(X.T@(C@(X@N))) + (mu/4)*(frobenius_norm(N-X.T@X)**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function with vector argument\n",
    "\n",
    "The standard optimisation functions we will be using work only for cost functions that take a vector as the varying argument. Write a new function, ```cost_function_for_vector```, that takes $X$ represented as a vector of length $np$ rather than a matrix of dimensions $n\\times p$. What arguments will this function take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_for_vector(Xvec, C, N, mu):\n",
    "    return cost_function_for_matrix(Xvec.reshape(C.shape[0], N.shape[0]),\n",
    "                                   C, N, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function\n",
    "\n",
    "At this point, we have two main choices in how we minimise the cost function using gradient descent functions from ``scipy.optimize``. First, we can use ``fmin``, which takes a function to minimize and an initial value. Second, we can use ``fmin_bfgs``, which takes an additional argument: the gradient of the function. As a result, (we would expect to find that) ``fmin_bfgs`` has substantially faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing with ```fmin```\n",
    "\n",
    "Implement a function ```minimise_f_using_fmin``` that, for given values of $C$, $N$ and $\\mu$, finds the matrix $X$ that minimizes $f(X)$ using ``fmin``. You will likely need [the docs for ``fmin``](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html). Check if your function converges for some (randomly generated) values of $C$, $N$ and $\\mu$.\n",
    "\n",
    "Summary of the docs: if you have a cost function $g(x, y)$ with a fixed value of $y$ and wish to find the value of $x$ that minimizes it, the syntax for calling ``fmin`` would be ``fmin(g, x0, args=(y))`` where ``x0`` is an initial guess for the value of $x$, and ``args=(y)`` gives ``fmin`` the rest of the values to pass to the cost function. Note that it is necessary that the variable that can change is the first argument to the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "def minimize_f_using_fmin(Xg, C, N, mu):\n",
    "    return opt.fmin(cost_function_for_vector, Xg, \n",
    "                    args=(C, N, mu), full_output=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.463333\n",
      "         Iterations: 761\n",
      "         Function evaluations: 1176\n",
      "Optimal X:\n",
      " [[-0.55754081  0.20110317]\n",
      " [ 0.22286015 -0.8939434 ]\n",
      " [ 0.5531214   0.56289308]]\n",
      "Min function value: -0.46333302519264374\n",
      "Number of iterations: 1176\n"
     ]
    }
   ],
   "source": [
    "# Testing fmin optimization\n",
    "\n",
    "np.random.seed(100)\n",
    "n = 3\n",
    "p = 2\n",
    "\n",
    "C = np.random.rand(n,n)\n",
    "C = C+C.T\n",
    "N = np.diag(np.random.rand(p))\n",
    "mu = frobenius_norm(C)\n",
    "Xg = np.random.rand(n*p)\n",
    "\n",
    "fmin = minimize_f_using_fmin(Xg,C,N,mu)\n",
    "print(\"Optimal X:\\n\", fmin[0].reshape(n,p))\n",
    "print(\"Min function value:\", fmin[1])\n",
    "print(\"Number of iterations:\", fmin[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the gradient of the cost function\n",
    "\n",
    "To use ``fmin_bfgs``, which is substantially more time efficient, we need to compute the gradient of $f(X)$ with respect to $X$. Calculate this gradient, then implement a function to calculate it. You may want to use Sam Roweis' [Matrix Identities](https://cs.nyu.edu/~roweis/notes/matrixid.pdf) and/or the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) as a reference for matrix calculus. As our cost function uses its main argument $X$ represented as a vector, also implement a function ```gradient_for_vector``` which returns the gradient represented as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "\n",
    "Cost function $f(X)$ can be rewritten as:\n",
    "\\begin{align*}\n",
    "    f(X) &= \\frac{1}{2} \\trace{X^T C X N} + \\frac{\\mu}{4} \\Norm{N - X^T X}^2_F \\\\\n",
    "    &= \\frac{1}{2} \\trace{X^T C X N} + \\frac{\\mu}{4} \\big( \\trace{N^\\top N} - \\trace{N^\\top X^\\top X} - \\trace{X^\\top X N} + \\trace{X^\\top XX^\\top X} \\big) \\\\\n",
    "    &= \\frac{1}{2} \\trace{X^T C X N} + \\frac{\\mu}{4} \\big( \\trace{N^\\top N} - \\trace{(N^\\top X^\\top X)^\\top} - \\trace{X^\\top X N} + \\trace{X^\\top XX^\\top X} \\big) \\\\\n",
    "    &= \\frac{1}{2} \\trace{X^T C X N} + \\frac{\\mu}{4} \\big( \\trace{N^\\top N} - \\trace{X^\\top X N} - \\trace{X^\\top X N} + \\trace{X^\\top XX^\\top X} \\big)\n",
    "\\end{align*}\n",
    "Thus,\n",
    "\\begin{align*}\n",
    "    \\nabla_{X} f &= \\frac{1}{2} (CXN + C^\\top X N^\\top) + \\frac{\\mu}{4} \\big( - (XN + XN^\\top) - (XN + XN^\\top ) + 4XX^\\top X \\big) \\\\\n",
    "    &= \\frac{1}{2} (CXN + C X N) + \\frac{\\mu}{4} \\big( - (XN + XN) - (XN + XN) + 4XX^\\top X \\big) \\\\\n",
    "    &= CXN + \\frac{\\mu}{4} (-4 XN + 4XX^\\top X) \\\\\n",
    "    &= CXN - \\mu X(N - X^\\top X)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_for_vector(X, C, N, mu):\n",
    "    X = X.reshape(C.shape[0], N.shape[0]) #reshape X to matrix\n",
    "    return (C@X@N - mu * (X@(N - X.T @ X))).reshape(C.shape[0]*N.shape[0],) #reshape back tp vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing the cost function using the gradient\n",
    "\n",
    "Write a function ```minimise_f_using_fmin_bfgs``` to minimise $f(X)$ using ```fmin_bfgs```. Have a look at the docs to find the correct syntax. Again, have a try of your function to check that it converges.\n",
    "\n",
    "* bfgs documentation: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_bfgs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_f_using_bfgs(Xg, C, N, mu):\n",
    "    return opt.fmin_bfgs(cost_function_for_vector, Xg,\n",
    "                         fprime=gradient_for_vector, \n",
    "                         args=(C,N,mu), \n",
    "                         full_output=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.463333\n",
      "         Iterations: 27\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n",
      "Optimal X:\n",
      " [[ 0.55753844  0.20111853]\n",
      " [-0.22287788 -0.89393062]\n",
      " [-0.55314803  0.56290324]]\n",
      "Min function value: -0.46333302750197697\n",
      "Number of function calls: 34\n"
     ]
    }
   ],
   "source": [
    "# Testing fmin bfgs optimization\n",
    "\n",
    "np.random.seed(100)\n",
    "n = 3\n",
    "p = 2\n",
    "\n",
    "C = np.random.rand(n,n)\n",
    "C = C+C.T\n",
    "N = np.diag(np.random.rand(p))\n",
    "mu = frobenius_norm(C)\n",
    "Xg = np.random.rand(n*p)\n",
    "\n",
    "\n",
    "fmin_bfgs = minimize_f_using_bfgs(Xg, C, N, mu)\n",
    "optX_bfgs = fmin_bfgs[0].reshape(n,p)\n",
    "print(\"Optimal X:\\n\", optX_bfgs)\n",
    "print(\"Min function value:\", fmin_bfgs[1])\n",
    "print(\"Number of function calls:\", fmin_bfgs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for convergence\n",
    "\n",
    "We wish to check whether ``fmin_bfgs`` is actually faster than ``fmin``.\n",
    "\n",
    "First, we need a way of randomly generating input parameters for our cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of a random matrix $C$ with given eigenvalues\n",
    "\n",
    "A diagonal matrix has the nice property that the eigenvalues can be directly read off\n",
    "the diagonal.\n",
    "\n",
    "* Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues, \n",
    "how many different diagonal matrices have the same set of eigenvalues?\n",
    "\n",
    "* Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues,\n",
    "how many different matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a set of $ n $ distinct real eigenvalues $ \\mathcal{E} = \\{e_1, \\dots, e_n \\} $, \n",
    "write a Python function ```random_matrix_from_eigenvalues``` which takes a list of\n",
    "eigenvalues $ E $ and returns a random symmetric matrix $ C $ having the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "\n",
    "* For a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues, there are $n!-1$ diagonal matrices with the same set of eigenvalues.\n",
    "* For a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues, there are infinitely many different matrices with the same set of eigenvalues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate\n",
    "def random_matrix_from_eigenvalues(e):\n",
    "    n = len(e)\n",
    "    A = np.random.rand(n,n)\n",
    "    Q,R = np.linalg.qr(A)\n",
    "    D = np.diag(e)\n",
    "    C = Q@D@Q.T\n",
    "#     print(\"C:\\n\",C)\n",
    "#     print(\"Generated eigenvalues:\",e)\n",
    "#     print(\"Eigenvalues of C:\",np.linalg.eigvals(C))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Above function generates random matrix $C$ is generated using matrix equation $C = QDQ^{\\top}$.\n",
    "\n",
    "We start with $C = ADA^{-1}$ where $A$ is an invertible matrix, and diagonal elements of $D$ are eigenvalues of $C$ and $A$.  \n",
    "\n",
    "A random orthogonal matrix $Q$ is then generated using QR decomposition, and then assigned to $A:=Q$, and because of orthogonality of columns in $Q$, then $Q^{-1}=Q^{\\top}$, therefore  $C = QDQ^{-1} = QDQ^{\\top}$\n",
    "\n",
    "References:  \n",
    "* https://www.mapleprimes.com/questions/40024-Creating-A-Matrix-By-Setting-Eigenvalues  \n",
    "* QR decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking convergence time\n",
    "\n",
    "Is ``fmin_bfgs`` actually faster than ``fmin``? Write some code to find out, using ```time.clock()```.\n",
    "\n",
    "Make sure to check this for relatively small and relatively large values of $n$ and $p$. Use ``random_matrix_from_eigenvalues`` to generate your $C$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "=========  minimize_f_using_fmin(initialise_low_dimensional_data)  =========\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 439\n",
      "         Function evaluations: 681\n",
      "run_time : 0.11900199999999961\n",
      "============================================================================\n",
      "=======  minimize_f_using_fmin(initialise_higher_dimensional_data)  ========\n",
      "============================================================================\n",
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "run_time : 6.001239999999999\n",
      "============================================================================\n",
      "=========  minimize_f_using_bfgs(initialise_low_dimensional_data)  =========\n",
      "============================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (6,6) and (1,6) not aligned: 6 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-64aa6a525437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_time :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mrun_and_time_all_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-64aa6a525437>\u001b[0m in \u001b[0;36mrun_and_time_all_tests\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minit_routine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"(X0,C,N,mu)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_time :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-44eb7926c044>\u001b[0m in \u001b[0;36mminimize_f_using_bfgs\u001b[0;34m(Xg, C, N, mu)\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_for_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                          full_output=1)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfmin_bfgs\u001b[0;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback)\u001b[0m\n\u001b[1;32m    907\u001b[0m             'return_all': retall}\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0mgnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvecnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgnorm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mgtol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         \u001b[0mpk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (6,6) and (1,6) not aligned: 6 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "np.random.seed(1)\n",
    "def initialise_low_dimensional_data():\n",
    "    \"\"\"Initialise the data, low dimensions\"\"\"\n",
    "    n = 3\n",
    "    p = 2\n",
    "    mu = 2.7\n",
    "\n",
    "    N = np.matrix(np.diag([2.5, 1.5]))\n",
    "    E = [1, 2, 3]\n",
    "    C = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "\n",
    "def initialise_higher_dimensional_data():\n",
    "    \"\"\"Initialise the data, higher dimensions\"\"\"\n",
    "    n  = 20\n",
    "    p  =  5\n",
    "    mu = p + 0.5\n",
    "\n",
    "    N = np.matrix(np.diag(np.arange(p, 0, -1)))\n",
    "    E = np.arange(1, n+1)\n",
    "    C = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "def pretty_printing(task_string):\n",
    "    line_length  = 76\n",
    "    spaces       = 2\n",
    "    left_padding = (line_length - len(task_string)) // 2\n",
    "    right_padding = line_length - left_padding - len(task_string)\n",
    "    print(\"=\" * line_length)\n",
    "    print(\"=\" * (left_padding - spaces) + \" \" * spaces + task_string + \\\n",
    "            \" \" * spaces + \"=\" * (right_padding - spaces))\n",
    "    print(\"=\" * line_length)    \n",
    "\n",
    "def run_and_time_all_tests():\n",
    "    \"\"\"Run all test and time them using a list of function names\"\"\"\n",
    "    List_of_Test_Names = [\"minimize_f_using_fmin\",\n",
    "                 \"minimize_f_using_bfgs\"]\n",
    "\n",
    "    List_of_Initialisations = [\"initialise_low_dimensional_data\",\n",
    "                               \"initialise_higher_dimensional_data\"]\n",
    "\n",
    "    for test_name in List_of_Test_Names:\n",
    "        for init_routine in List_of_Initialisations:\n",
    "            task_string  = test_name + \"(\" + init_routine + \")\"\n",
    "            pretty_printing(task_string)\n",
    "\n",
    "            start = time.clock()\n",
    "            C, N, mu, n, p, X0 = globals()[init_routine]()\n",
    "            exec(test_name+\"(X0,C,N,mu)\")\n",
    "            run_time = time.clock() - start\n",
    "            print(\"run_time :\", run_time)\n",
    "\n",
    "run_and_time_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start bfgs...\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.014929\n",
      "         Iterations: 99\n",
      "         Function evaluations: 104\n",
      "         Gradient evaluations: 104\n",
      "bfgs time: 0.03460900000000011\n",
      "Min function value w/ bfgs: 0.014928879562503591\n",
      "\n",
      "Start fmin...\n",
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "fmin time: 0.3741439999999998\n",
      "Min function value w/ fmin: 0.014983180890579224\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "p = 3\n",
    "\n",
    "# np.random.seed(100)\n",
    "Xg = np.random.rand(n*p)\n",
    "N = np.diag(np.random.rand(p))\n",
    "e = np.random.rand(n)\n",
    "C = random_matrix_from_eigenvalues(e)\n",
    "mu = np.amin(e[:(len(e)-p)])\n",
    "\n",
    "print(\"\\nStart bfgs...\")\n",
    "start = time.clock()\n",
    "fmin_bfgs = minimize_f_using_bfgs(Xg, C, N, mu)\n",
    "bfgs_time = time.clock() - start\n",
    "print(\"bfgs time:\", bfgs_time)\n",
    "print(\"Min function value w/ bfgs:\", fmin_bfgs[1])\n",
    "\n",
    "print(\"\\nStart fmin...\")\n",
    "start = time.clock()\n",
    "fmin = minimize_f_using_fmin(Xg,C,N,mu)\n",
    "fmin_time = time.clock() - start\n",
    "print(\"fmin time:\", fmin_time)\n",
    "print(\"Min function value w/ fmin:\", fmin[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minima of $f(X)$\n",
    "\n",
    "Compare the columns $x_1,\\dots, x_p$ of the matrix $X^\\star$ which minimises $ f(X) $ \n",
    "\\begin{equation}\n",
    "  X^\\star = \\argmin_{X \\in \\RR^{n \\times p}} f(X)\n",
    "\\end{equation}\n",
    "\n",
    "with the eigenvectors related to the smallest eigenvalues of $ C $.\n",
    "\n",
    "What do you believe this means about $f(X)$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "\n",
    "code references:\n",
    "* get eigenvalues and eigenvectors `numpy.linalg.eig`: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html\n",
    "* eigenvalue-eigenvector sorting: https://stackoverflow.com/questions/8092920/sort-eigenvalues-and-associated-eigenvectors-after-using-numpy-linalg-eig-in-pyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "==========  Comparing X_at_min and Eigenvalues for random values  ==========\n",
      "============================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (6,6) and (1,6) not aligned: 6 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5e8f1c85a650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mshow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_at_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mshow_comparision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-5e8f1c85a650>\u001b[0m in \u001b[0;36mshow_comparision\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mpretty_printing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Comparing X_at_min and Eigenvalues for random values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialise_low_dimensional_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mX_at_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize_f_using_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mshow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_at_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-44eb7926c044>\u001b[0m in \u001b[0;36mminimize_f_using_bfgs\u001b[0;34m(Xg, C, N, mu)\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_for_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                          full_output=1)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfmin_bfgs\u001b[0;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback)\u001b[0m\n\u001b[1;32m    907\u001b[0m             'return_all': retall}\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0mgnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvecnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgnorm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mgtol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         \u001b[0mpk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (6,6) and (1,6) not aligned: 6 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "np.random.seed(5)\n",
    "\n",
    "def pretty_printing(task_string):\n",
    "    line_length  = 76\n",
    "    spaces       = 2\n",
    "    left_padding = (line_length - len(task_string)) // 2\n",
    "    right_padding = line_length - left_padding - len(task_string)\n",
    "    print(\"=\" * line_length)\n",
    "    print(\"=\" * (left_padding - spaces) + \" \" * spaces + task_string + \\\n",
    "            \" \" * spaces + \"=\" * (right_padding - spaces))\n",
    "    print(\"=\" * line_length)  \n",
    "    \n",
    "def normalize_columns(A):\n",
    "    \"\"\"Normalise the columns of a 2-D array or matrix to length one\n",
    "    A - array or matrix (which will be modified)\n",
    "    \"\"\"\n",
    "    if A.ndim != 2:\n",
    "        raise ValueError(\"A is not a 2-D array\")\n",
    "\n",
    "    number_of_columns = A.shape[1]\n",
    "    for i in range(number_of_columns):\n",
    "        A[:,i] /= np.linalg.norm(A[:,i], ord=2)\n",
    "\n",
    "\n",
    "def show_results(X_at_min, C):\n",
    "    \"\"\"Display the found arg min and compare with eigenvalues of C\n",
    "    X_at_min -- arguement at minimum found\n",
    "    C        -- symmetric matrix\n",
    "    \"\"\"\n",
    "    n,p = X_at_min.shape\n",
    "\n",
    "    normalize_columns(X_at_min)\n",
    "\n",
    "    # Get the eigenvectors belonging to the smallest eigenvalues\n",
    "    Eigen_Values, Eigen_Vectors = np.linalg.eig(C)\n",
    "    Permutation = Eigen_Values.argsort()\n",
    "    Smallest_Eigenvectors = Eigen_Vectors[:, Permutation[:p]]\n",
    "\n",
    "    if n < 10:\n",
    "        print(\"X_at_min               :\\n\", X_at_min)\n",
    "        print()\n",
    "        print(\"Smallest_Eigenvectors  :\\n\", Smallest_Eigenvectors)\n",
    "        print()\n",
    "    else:\n",
    "        Project_into_Eigenvectorspace = \\\n",
    "          Smallest_Eigenvectors * Smallest_Eigenvectors.T * X_at_min\n",
    "        Normal_Component = X_at_min - Project_into_Eigenvectorspace\n",
    "\n",
    "        print(\"norm(Normal_Component)/per entry :\", \\\n",
    "            np.linalg.norm(Normal_Component, ord=2) / float(n*p))\n",
    "\n",
    "def show_comparision(num=3):\n",
    "    for _ in range(num):\n",
    "        pretty_printing(\"Comparing X_at_min and Eigenvalues for random values\")\n",
    "        C, N, mu, n, p, X0 = initialise_low_dimensional_data()\n",
    "        X_at_min = minimize_f_using_bfgs(X0,C,N,mu)\n",
    "        show_results(X_at_min, C)\n",
    "\n",
    "show_comparision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfgs optimized X:\n",
      "[[-0.43033632  0.47752377 -0.2200242 ]\n",
      " [-0.33325752  0.21646084  0.41024105]\n",
      " [-0.09914416  0.14742203 -0.22746719]\n",
      " [-0.137262    0.15859336 -0.14226756]]\n",
      "fmin optimized X:\n",
      "[[ 0.3054756   0.21799689  0.2250257 ]\n",
      " [ 0.28563533 -0.41251024 -0.40378771]\n",
      " [ 0.08595752  0.22762328  0.22791211]\n",
      " [ 0.32293155  0.1418696   0.14327448]]\n",
      "min eigvenvalues:\n",
      "[0.03647606 0.10514769 0.38194344 0.89041156]\n",
      "corresponding eigenvectors:\n",
      "[[ 0.2202  0.442   0.2568  0.067 ]\n",
      " [-0.41    0.3346 -0.1255 -0.097 ]\n",
      " [ 0.2275  0.0946 -0.2359 -0.4692]\n",
      " [ 0.1423  0.1288 -0.3818  0.3668]]\n"
     ]
    }
   ],
   "source": [
    "X_opt_bfgs = fmin_bfgs[0]\n",
    "X_opt_fmin = fmin[0]\n",
    "\n",
    "X_opt_bfgs = np.reshape(X_opt_bfgs, (n,p))\n",
    "X_opt_fmin = np.reshape(X_opt_fmin, (n,p))\n",
    "\n",
    "# normalize columns of optimized X\n",
    "for i in range(X_opt_bfgs.shape[1]):\n",
    "    normb = np.linalg.norm(X_opt_bfgs[:,i], ord=1)\n",
    "    X_opt_bfgs[:,i] = X_opt_bfgs[:,i] / normb\n",
    "    normf = np.linalg.norm(X_opt_fmin[:,i], ord=1)\n",
    "    X_opt_fmin[:,i] = X_opt_fmin[:,i] / normf    \n",
    "    \n",
    "# get eigenvalues and eigernvectors\n",
    "pmineigval, pmineigvec = np.linalg.eig(C)\n",
    "# sort eigenvalues and eigenvectors\n",
    "permut = pmineigval.argsort()\n",
    "pmineigval = pmineigval[permut]\n",
    "pmineigvec = pmineigvec[:,permut]\n",
    "    \n",
    "# normalize columns of eigenvectors of C\n",
    "for i in range(pmineigvec.shape[1]):\n",
    "    normb = np.linalg.norm(pmineigvec[:,i], ord=1)\n",
    "    pmineigvec[:,i] = pmineigvec[:,i] / normb\n",
    "    normf = np.linalg.norm(pmineigvec[:,i], ord=1)\n",
    "    pmineigvec[:,i] = pmineigvec[:,i] / normf\n",
    "    \n",
    "print(\"bfgs optimized X:\")\n",
    "print(X_opt_bfgs)\n",
    "print(\"fmin optimized X:\")\n",
    "print(X_opt_fmin)\n",
    "\n",
    "print(\"min eigvenvalues:\")\n",
    "print(pmineigval)\n",
    "print(\"corresponding eigenvectors:\")\n",
    "print(np.around(pmineigvec, decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
