{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will practice minimising a cost function with gradient descent.\n",
    "\n",
    "### Assumed knowledge\n",
    "- Linear algebra (see Sam Roweis' notes, linked below, for matrix calculus tips)\n",
    "- Python programming\n",
    "- Preferably: Using numpy for matrix calculations (precourse material)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Using numpy ndarrays for matrix calculations\n",
    "- Using scipy.optimise routines to minimise a cost function, with and without a gradient\n",
    "- Randomly generating input values for testing\n",
    "\n",
    "## Pre-lab notes\n",
    "In this lab, you will apply linear algebra to to minimise a cost function in three steps: implementing the cost function, implementing a gradient function, and applying gradient descent. We will be doing this to solve problems throughout the course.\n",
    "\n",
    "As in all labs, feel free to skip questions if you get stuck, and ask your tutor if you have any questions!\n",
    "\n",
    "A note on style: in this course we emphasise *functional decomposition* in code style. Avoid using global variables, and remember that often splitting code off into separate functions can make it more readable and testable. (Jupyter notebooks let you call functions defined in previous cells.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up python environment (this cell contains Latex macros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as opt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *cost function* or *loss function* is the function we want to minimize in a given problem. For example, it might measure the error between the indicator values our models predicts and the true values for the training data. In this lab, we consider a toy example. We will define a cost function $f(X)$ where $X$ is a $n\\times p$ matrix.\n",
    "\n",
    "If $A$ is a square matrix, then we write $\\trace{A}$ for its trace. Let $ \\Norm{A}_F = \\sqrt{\\trace{A^T A}} $, the *Frobenius norm* of a matrix.\n",
    "\n",
    "Let our cost function $f(X)$ be defined for $n\\times p$ matrices $X$ as follows. Let $C$ be a fixed symmetric $n\\times n$ matrix (so $C = C^T$). Let $\\mu$ be a scalar that is larger than the $p^{th}$ smallest eigenvalue of $C$. Let $N$ be a diagonal $p\\times p$ matrix with distinct positive entries on the diagonal.\n",
    "\n",
    "The cost function is defined as\n",
    "\\begin{equation}\n",
    "  f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F\n",
    "\\end{equation}\n",
    "where $ X \\in \\RR^{n \\times p} $, $ n \\ge p $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenious Norm\n",
    "\n",
    "Implement a Python function ```frobenius_norm``` which accepts an arbitrary matrix $ A $ and returns\n",
    "$ \\Norm{A}_F $ using the formula given. (Use ```numpy.trace``` and ```numpy.sqrt```.) We represent matrices and vectors as numpy ndarrays.\n",
    "1. Given a matrix $ A \\in \\RR^{n \\times p} $, what is the complexity of your implementation of ```frobenius_norm```\n",
    "using the formula above?\n",
    "2. Can you come up with a faster implementation, if you were additionally told that $ p \\ge n $ ?\n",
    "\n",
    "Extension: Can you find an even faster implementation than in 1. and 2.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\n",
    "1. The formula above invovles multiplying the matrices as $A^TA$ and then taking the trace. The complexity of the entire operation comes from multipying the matrices, which is of complexity $O(p^2n)$.\n",
    "\n",
    "2. If $p\\geq n$, then as $ \\trace{A^T A} = \\trace{A A^T}$ we can multiply the matrices as $AA^T$ to get a complexity of $O(n^2p)$. Which property of traces did we use to achieve this identity?\n",
    "\n",
    "Extension: Note that $\\|A\\|_F=\\sqrt{\\sum\\limits_{i=1}^p\\sum\\limits_{j=1}^n A_{j,i}^2}$, which has a complexity of $O(np)$. To see this, note that\n",
    "\\begin{align*}\n",
    "  \\trace{A^T A} &= \\sum_{i=1}^p (A^T A)_{i,i} \\\\\n",
    "                &= \\sum_{i=1}^p \\sum_{j=1}^n \\underbrace{(A^T)_{i,j}}_{=A_{j, i}} A_{j,i}\\\\\n",
    "                &= \\sum_{i=1}^p \\sum_{j=1}^n A_{j,i}^2.\n",
    "\\end{align*}\n",
    "\n",
    "We will use this method to implement the Frobenius Norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85762625 0.53286482 0.90607835]\n",
      " [0.78284506 0.91066499 0.0643526 ]\n",
      " [0.17403402 0.46824711 0.28689979]\n",
      " [0.72233001 0.33099489 0.99187648]\n",
      " [0.62327875 0.58373964 0.36672811]]\n",
      "2.4693022946168357\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "def frobenius_norm(A):\n",
    "    \"\"\"Calculate the Frobenius norm of an array or matrix.\n",
    "    A -- array or matrix\n",
    "    \"\"\"\n",
    "    return np.sqrt((np.asarray(A)**2).sum(axis=None))\n",
    "\n",
    "M = np.random.rand(5,3)\n",
    "print(M)\n",
    "print(frobenius_norm(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function, ```cost_function_for_matrix```, which implements the function $f(X)$ defined above.\n",
    "\n",
    "Hint: What should the arguments to this function be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def cost_function_for_matrix(X, C, N, mu):\n",
    "    \"\"\"\n",
    "    Calculate the cost function at point X given as a matrix.\n",
    "    X -- current point in matrix form\n",
    "    C -- symmetric matrix\n",
    "    N -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    returns the value of the cost function as a scalar.\n",
    "    \"\"\"\n",
    "    if not isinstance(X, np.matrix):\n",
    "        raise TypeError(\"X is not a matrix\")\n",
    "\n",
    "    if not isinstance(C, np.matrix):\n",
    "        raise TypeError(\"C is not a matrix\")\n",
    "\n",
    "    if not isinstance(N, np.matrix):\n",
    "        raise TypeError(\"N is not a matrix\")\n",
    "\n",
    "    r1 = 0.5  * np.trace(X.T * C * X * N)\n",
    "    r2 = 0.25 * mu * frobenius_norm(N - X.T * X)**2\n",
    "    return r1 + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function with vector argument\n",
    "\n",
    "The standard optimisation functions we will be using work only for cost functions that take a vector as the varying argument. Write a new function, ```cost_function_for_vector```, that takes $X$ represented as a vector of length $np$ rather than a matrix of dimensions $n\\times p$. What arguments will this function take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def cost_function_for_vector(X, C, N, mu, n, p):\n",
    "    \"\"\"Calculate the cost function at point X given as 1-D array\n",
    "    X  -- current point as 1-D array\n",
    "    C  -- symmetric matrix\n",
    "    N  -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    n  -- row dimension of X\n",
    "    p  -- column dimension of X\n",
    "    returns the value of the cost function as a scalar\n",
    "    \"\"\"\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        raise TypeError(\"X is not a matrix\")\n",
    "\n",
    "    if X.ndim != 1:\n",
    "        raise ValueError(\"X is not a 1-D vector\")\n",
    "\n",
    "    Xmatrix = np.matrix(X.reshape((n, p)))\n",
    "    return cost_function_for_matrix(Xmatrix, C, N, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function\n",
    "\n",
    "At this point, we have two main choices in how we minimise the cost function using gradient descent functions from ``scipy.optimize``. First, we can use ``fmin``, which takes a function to minimize and an initial value. Second, we can use ``fmin_bfgs``, which takes an additional argument: the gradient of the function. As a result, (we would expect to find that) ``fmin_bfgs`` has substantially faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing with ```fmin```\n",
    "\n",
    "Implement a function ```minimise_f_using_fmin``` that, for given values of $C$, $N$ and $\\mu$, finds the matrix $X$ that minimizes $f(X)$ using ``fmin``. You will likely need [the docs for ``fmin``](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html). Check if your function converges for some (randomly generated) values of $C$, $N$ and $\\mu$.\n",
    "\n",
    "Summary of the docs: if you have a cost function $g(x, y)$ with a fixed value of $y$ and wish to find the value of $x$ that minimizes it, the syntax for calling ``fmin`` would be ``fmin(g, x0, args=(y))`` where ``x0`` is an initial guess for the value of $x$, and ``args=(y)`` gives ``fmin`` the rest of the values to pass to the cost function. Note that it is necessary that the variable that can change is the first argument to the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.463333\n",
      "         Iterations: 554\n",
      "         Function evaluations: 857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-0.55754315,  0.20111999],\n",
       "        [ 0.22288385, -0.89391836],\n",
       "        [ 0.55312147,  0.56292356]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def minimise_f_using_fmin(C, N, mu, n, p, X0):\n",
    "    \"\"\"Run minimisation with simplex algorithm.\"\"\"\n",
    "\n",
    "    X_at_min = opt.fmin(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 args=(C, N, mu, n, p))\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    return X_at_min\n",
    "\n",
    "def randomly_generate_values(n, p):\n",
    "    C = np.random.rand(n,n)\n",
    "    C = np.matrix(C+C.T)\n",
    "    N = np.matrix(np.diag(np.random.rand(p)))\n",
    "    mu = frobenius_norm(C)\n",
    "    X0 = np.random.randn(n,p)\n",
    "    return C, N, mu, X0\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "n = 3\n",
    "p = 2\n",
    "C, N, mu, X0 = randomly_generate_values(n, p)\n",
    "minimise_f_using_fmin(C, N, mu, n, p, X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the gradient of the cost function\n",
    "\n",
    "To use ``fmin_bfgs``, which is substantially more time efficient, we need to compute the gradient of $f(X)$ with respect to $X$. Calculate this gradient, then implement a function to calculate it. You may want to use Sam Roweis' [Matrix Identities](https://cs.nyu.edu/~roweis/notes/matrixid.pdf) and/or the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) as a reference for matrix calculus. As our cost function uses its main argument $X$ represented as a vector, also implement a function ```gradient_for_vector``` which returns the gradient represented as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The cost function was defined as\n",
    "\\begin{equation}\n",
    "  f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F\n",
    "\\end{equation}\n",
    "where $ X \\in \\RR^{n \\times p} $, $ n \\ge p $.\n",
    "\n",
    "You should be able to show that \n",
    "\\begin{align*}\n",
    "    \\nabla_X \\trace{X^TCXN} \n",
    "        &= CXN + C^TxN^T\\\\\n",
    "        &= 2CXN \\tag{as $C$ and $N$ are symmetric}\n",
    "\\end{align*}\n",
    "and that\n",
    "\\begin{align*}\n",
    "    \\nabla_X \\Norm{N - X^T X}^2_F \n",
    "        &= -2XN^T-2XN + 4 XX^TX\\\\\n",
    "        &= -4XN+4XX^TX \\tag{as $N$ is symmetric}\\\\\n",
    "        &= -4X(N-X^TX).\n",
    "\\end{align*}\n",
    "\n",
    "This can be done by calculating the partial derivative for an arbitrary $X_{ij}$ and generalising.\n",
    "\n",
    "For example, note that\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial X_{ij}} \\trace{X^TCXN} \n",
    "        &= \\frac{\\partial}{\\partial X_{ij}} \\sum_k\\sum_l\\sum_m\\sum_n X_{lk}C_{lm}X_{mn}N_{nk}\\\\\n",
    "        &= \\sum_m\\sum_n C_{im}X_{mn}N_{nj} + \\sum_k\\sum_l X_{lk}C_{li}N_{jk}\\\\\n",
    "        &= (CXN)_{ij} + (C^TXN^T)_{ij}\n",
    "\\end{align*}\n",
    "so $$\\nabla_X \\trace{X^TCXN} = CXN + C^TXN^T.$$\n",
    "\n",
    "To check this, from the [matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) we have the identity (page 13, equation 117)\n",
    "$$\\nabla_X\\trace{X^TBXC} = BXC+B^TXC^T$$\n",
    "so it follows that\n",
    "$$\\nabla_X \\trace{X^TCXN} = CXN + C^TXN^T.$$\n",
    "\n",
    "Thus we have that\n",
    "\\begin{align*}\n",
    "    \\nabla_X f(X)\n",
    "        &= CXN - \\mu X(N-X^TX).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def gradient_for_matrix(X, C, N, mu):\n",
    "    \"\"\"Calculate the gradient for the cost function at a point X\n",
    "    X  -- current point in matrix form\n",
    "    C  -- symmetric matrix\n",
    "    N  -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    returns the gradient of the cost function as matrix\n",
    "    \"\"\"\n",
    "    gradient = C * X * N - mu * X * (N - X.T * X)\n",
    "    return gradient\n",
    "\n",
    "def gradient_for_vector(X, C, N, mu, n, p):\n",
    "    \"\"\"Calculate the gradient for the cost function at a point X\n",
    "    X  -- current point as 1-D array\n",
    "    C  -- symmetric matrix\n",
    "    N  -- diagonal matrix\n",
    "    mu -- scalar\n",
    "    n  -- row dimension of X\n",
    "    p  -- column dimension of X\n",
    "    returns the gradient of the cost function as 1-D array\n",
    "    \"\"\"\n",
    "    Xmatrix = np.matrix(X.reshape((n, p)))\n",
    "    gradient =  gradient_for_matrix(Xmatrix, C, N, mu)\n",
    "    return np.asarray(gradient).reshape((n*p,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing the cost function using the gradient\n",
    "\n",
    "Write a function ```minimise_f_using_fmin_bfgs``` to minimise $f(X)$ using ```fmin_bfgs```. Have a look at the docs to find the correct syntax. Again, have a try of your function to check that it converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.463333\n",
      "         Iterations: 17\n",
      "         Function evaluations: 19\n",
      "         Gradient evaluations: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-0.55754071, -0.20112249],\n",
       "        [ 0.22287441,  0.8939319 ],\n",
       "        [ 0.5531515 , -0.56290044]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "def minimise_f_using_fmin_bfgs(C, N, mu, n, p, X0):\n",
    "    \"\"\"Run minimisation with BFGS algorithm\"\"\"\n",
    "\n",
    "    X_at_min = opt.fmin_bfgs(cost_function_for_vector,\n",
    "                                 X0,\n",
    "                                 fprime=gradient_for_vector,\n",
    "                                 args=(C, N, mu, n, p))\n",
    "    X_at_min = np.matrix(X_at_min.reshape((n, p)))\n",
    "    return X_at_min\n",
    "    \n",
    "np.random.seed(100)\n",
    "n = 3\n",
    "p = 2\n",
    "C, N, mu, X0 = randomly_generate_values(n, p)\n",
    "minimise_f_using_fmin_bfgs(C, N, mu, n, p, X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for convergence\n",
    "\n",
    "We wish to check whether ``fmin_bfgs`` is actually faster than ``fmin``.\n",
    "\n",
    "First, we need a way of randomly generating input parameters for our cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of a random matrix $C$ with given eigenvalues\n",
    "\n",
    "A diagonal matrix has the nice property that the eigenvalues can be directly read off\n",
    "the diagonal. Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues, \n",
    "how many different diagonal matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues,\n",
    "how many different matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a set of $ n $ distinct real eigenvalues $ \\mathcal{E} = \\{e_1, \\dots, e_n \\} $, \n",
    "write a Python function ```random_matrix_from_eigenvalues``` which takes a list of\n",
    "eigenvalues $ E $ and returns a random symmetric matrix $ C $ having the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "There are $ n! $ permutations of diagonal elements, but infinitely many matrices\n",
    "with the same set of eigenvalues (can scale with any positive number).\n",
    "\n",
    "In order to construct a random matrix with given eigenvalues $\\lambda_i$, $i=1,\\dots,n$\n",
    "we first create a diagonal matrix $ \\Lambda $ with those eigenvalues on the\n",
    "diagonal. Then we can get another matrix $ A $ with the same eigenvalues as $ \\Lambda $\n",
    "if we apply an arbitrary nonsingular matrix $ B $ to get $ A = B \\Lambda B^{-1} $.\n",
    "(Can you prove that $ A $ and $ \\Lambda $ have the same set of eigenvalues?)\n",
    "\n",
    "If $ B $ is an orthogonal matrix $ Q $, then $ Q^{-1} = Q^T $ and therefore the above\n",
    "formula results in $ A = Q \\Lambda Q^T $ which is much faster to calculate then\n",
    "using the inverse of a matrix.\n",
    "\n",
    "How to get a random orthogonal matrix? We use the QR-decomposition of a matrix which \n",
    "decomposes every arbitrary matrix $ B $ into an orthogonal matrix $ Q $ and an \n",
    "upper-triangular matrix $ R $, $ B = Q R $.\n",
    "\n",
    "The algorithm therefore is\n",
    "1. Choose a random matrix $ B $ by randomly choosing the elements of $ B $.\n",
    "2. Calculate the QR-decomposition $ B = Q R $. (Check that $ B $ is nonsingular\n",
    "      by checking the diagonal of $ R $ has nonzero elements.)\n",
    "3. Calculate $ A =  Q \\Lambda Q^T $, the wanted arbitrary matrix with the\n",
    "      same eigenvalues as $ \\Lambda $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def random_matrix_from_eigenvalues(E):\n",
    "    \"\"\"Create a square random matrix with a given set of eigenvalues\n",
    "    E -- list of eigenvalues\n",
    "    return the random matrix\n",
    "    \"\"\"\n",
    "    n    = len(E)\n",
    "    # Create a random orthogonal matrix Q via QR decomposition\n",
    "    # of a random matrix A\n",
    "    A    = np.matrix(np.random.rand(n,n))\n",
    "    Q, R = np.linalg.qr(A)\n",
    "    #  similarity transformation with orthogonal\n",
    "    #  matrix leaves eigenvalues intact\n",
    "    return Q * np.diag(E) * Q.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking convergence time\n",
    "\n",
    "Is ``fmin_bfgs`` actually faster than ``fmin``? Write some code to find out, using ```time.clock()```.\n",
    "\n",
    "Make sure to check this for relatively small and relatively large values of $n$ and $p$. Use ``random_matrix_from_eigenvalues`` to generate your $C$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "=========  minimise_f_using_fmin(initialise_low_dimensional_data)  =========\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 439\n",
      "         Function evaluations: 681\n",
      "run_time : 0.1660790000000003\n",
      "============================================================================\n",
      "=======  minimise_f_using_fmin(initialise_higher_dimensional_data)  ========\n",
      "============================================================================\n",
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "run_time : 7.041362\n",
      "============================================================================\n",
      "======  minimise_f_using_fmin_bfgs(initialise_low_dimensional_data)  =======\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 12\n",
      "         Function evaluations: 18\n",
      "         Gradient evaluations: 18\n",
      "run_time : 0.01038600000000045\n",
      "============================================================================\n",
      "=====  minimise_f_using_fmin_bfgs(initialise_higher_dimensional_data)  =====\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 40.727273\n",
      "         Iterations: 128\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 137\n",
      "run_time : 0.2757680000000011\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "np.random.seed(1)\n",
    "def initialise_low_dimensional_data():\n",
    "    \"\"\"Initialise the data, low dimensions\"\"\"\n",
    "    n = 3\n",
    "    p = 2\n",
    "    mu = 2.7\n",
    "\n",
    "    N = np.matrix(np.diag([2.5, 1.5]))\n",
    "    E = [1, 2, 3]\n",
    "    C = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "\n",
    "def initialise_higher_dimensional_data():\n",
    "    \"\"\"Initialise the data, higher dimensions\"\"\"\n",
    "    n  = 20\n",
    "    p  =  5\n",
    "    mu = p + 0.5\n",
    "\n",
    "    N = np.matrix(np.diag(np.arange(p, 0, -1)))\n",
    "    E = np.arange(1, n+1)\n",
    "    C = random_matrix_from_eigenvalues(E)\n",
    "    X0 = np.random.rand(n*p)\n",
    "\n",
    "    return C, N, mu, n, p, X0\n",
    "\n",
    "def pretty_printing(task_string):\n",
    "    line_length  = 76\n",
    "    spaces       = 2\n",
    "    left_padding = (line_length - len(task_string)) // 2\n",
    "    right_padding = line_length - left_padding - len(task_string)\n",
    "    print(\"=\" * line_length)\n",
    "    print(\"=\" * (left_padding - spaces) + \" \" * spaces + task_string + \\\n",
    "            \" \" * spaces + \"=\" * (right_padding - spaces))\n",
    "    print(\"=\" * line_length)    \n",
    "\n",
    "def run_and_time_all_tests():\n",
    "    \"\"\"Run all test and time them using a list of function names\"\"\"\n",
    "    List_of_Test_Names = [\"minimise_f_using_fmin\",\n",
    "                 \"minimise_f_using_fmin_bfgs\"]\n",
    "\n",
    "    List_of_Initialisations = [\"initialise_low_dimensional_data\",\n",
    "                               \"initialise_higher_dimensional_data\"]\n",
    "\n",
    "    for test_name in List_of_Test_Names:\n",
    "        for init_routine in List_of_Initialisations:\n",
    "            task_string  = test_name + \"(\" + init_routine + \")\"\n",
    "            pretty_printing(task_string)\n",
    "\n",
    "            start = time.clock()\n",
    "            C, N, mu, n, p, X0 = globals()[init_routine]()\n",
    "            exec(test_name+\"(C,N,mu,n,p,X0)\")\n",
    "            run_time = time.clock() - start\n",
    "            print(\"run_time :\", run_time)\n",
    "\n",
    "run_and_time_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minima of $f(X)$\n",
    "\n",
    "Compare the columns $x_1,\\dots, x_p$ of the matrix $X^\\star$ which minimises $ f(X) $ \n",
    "\\begin{equation}\n",
    "  X^\\star = \\argmin_{X \\in \\RR^{n \\times p}} f(X)\n",
    "\\end{equation}\n",
    "\n",
    "with the eigenvectors related to the smallest eigenvalues of $ C $.\n",
    "\n",
    "What do you believe this means about $f(X)$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "From the code below we see that the normalised columns of $X^\\star$ \n",
    "correspond to eigenvectors of $C$.\n",
    "\n",
    "In fact, the minimum of the given cost function are matrices $ X $ which contain\n",
    "the $ p $ eigenvectors of $ C $ which are associated with the $ p $ smallest\n",
    "eigenvalues of $ C $. The order of the eigenvector in the minimum $ X $\n",
    "is defined by the order of the diagonal elements in $ N $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "==========  Comparing X_at_min and Eigenvalues for random values  ==========\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 19\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 24\n",
      "X_at_min               :\n",
      " [[ 0.18249115 -0.9764946 ]\n",
      " [ 0.75515978  0.21391152]\n",
      " [ 0.62962741  0.02646037]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[-0.18249318 -0.97649328]\n",
      " [-0.75515939  0.21391723]\n",
      " [-0.6296273   0.0264629 ]]\n",
      "\n",
      "============================================================================\n",
      "==========  Comparing X_at_min and Eigenvalues for random values  ==========\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 14\n",
      "         Function evaluations: 19\n",
      "         Gradient evaluations: 19\n",
      "X_at_min               :\n",
      " [[ 0.37258559 -0.89968829]\n",
      " [ 0.85475654  0.42815364]\n",
      " [ 0.36134644 -0.08512011]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[-0.37258535 -0.89968729]\n",
      " [-0.85475642  0.42815549]\n",
      " [-0.36134696 -0.08512138]]\n",
      "\n",
      "============================================================================\n",
      "==========  Comparing X_at_min and Eigenvalues for random values  ==========\n",
      "============================================================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 3.962963\n",
      "         Iterations: 16\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 20\n",
      "X_at_min               :\n",
      " [[ 0.03116539 -0.36738293]\n",
      " [ 0.99951202  0.01341159]\n",
      " [ 0.00210664 -0.92997307]]\n",
      "\n",
      "Smallest_Eigenvectors  :\n",
      " [[-0.03116645 -0.36737991]\n",
      " [-0.99951199  0.01341466]\n",
      " [-0.00210563 -0.92997422]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "np.random.seed(5)\n",
    "def normalize_columns(A):\n",
    "    \"\"\"Normalise the columns of a 2-D array or matrix to length one\n",
    "    A - array or matrix (which will be modified)\n",
    "    \"\"\"\n",
    "    if A.ndim != 2:\n",
    "        raise ValueError(\"A is not a 2-D array\")\n",
    "\n",
    "    number_of_columns = A.shape[1]\n",
    "    for i in range(number_of_columns):\n",
    "        A[:,i] /= np.linalg.norm(A[:,i], ord=2)\n",
    "\n",
    "\n",
    "def show_results(X_at_min, C):\n",
    "    \"\"\"Display the found arg min and compare with eigenvalues of C\n",
    "    X_at_min -- arguement at minimum found\n",
    "    C        -- symmetric matrix\n",
    "    \"\"\"\n",
    "    n,p = X_at_min.shape\n",
    "\n",
    "    normalize_columns(X_at_min)\n",
    "\n",
    "    # Get the eigenvectors belonging to the smallest eigenvalues\n",
    "    Eigen_Values, Eigen_Vectors = np.linalg.eig(C)\n",
    "    Permutation = Eigen_Values.argsort()\n",
    "    Smallest_Eigenvectors = Eigen_Vectors[:, Permutation[:p]]\n",
    "\n",
    "    if n < 10:\n",
    "        print(\"X_at_min               :\\n\", X_at_min)\n",
    "        print()\n",
    "        print(\"Smallest_Eigenvectors  :\\n\", Smallest_Eigenvectors)\n",
    "        print()\n",
    "    else:\n",
    "        Project_into_Eigenvectorspace = \\\n",
    "          Smallest_Eigenvectors * Smallest_Eigenvectors.T * X_at_min\n",
    "        Normal_Component = X_at_min - Project_into_Eigenvectorspace\n",
    "\n",
    "        print(\"norm(Normal_Component)/per entry :\", \\\n",
    "            np.linalg.norm(Normal_Component, ord=2) / float(n*p))\n",
    "\n",
    "def show_comparision(num=3):\n",
    "    for _ in range(num):\n",
    "        pretty_printing(\"Comparing X_at_min and Eigenvalues for random values\")\n",
    "        C, N, mu, n, p, X0 = initialise_low_dimensional_data()\n",
    "        X_at_min = minimise_f_using_fmin_bfgs(C,N,mu,n,p,X0)\n",
    "        show_results(X_at_min, C)\n",
    "\n",
    "show_comparision()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
