{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the environment\n",
    "\n",
    "In this lab, we will train a neural network as a classifer.\n",
    "\n",
    "### Assumed knowledge\n",
    "- Neural networks (week 5 lectures)\n",
    "- Classifiers (week 3 and week 4 labs)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Implementing a neural network\n",
    "- Calculating back-propogation formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.529906Z",
     "start_time": "2019-03-20T08:29:27.346073Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We will be working with a similar dataset to the one used in the Classification lab.\n",
    "This is a census-income dataset, which shows income levels for people in the 1994 US Census. We will predict whether a person has $\\leq \\$50000$ or $> \\$50000$ income per year.\n",
    "\n",
    "Unlike in the Classification lab, this data is not linearly separable. That is, the linear classification techniques you learnt about in previous weeks are not effective on this data.\n",
    "\n",
    "The data is included with this notebook as `05-dataset.tsv`. Load the data into a NumPy array called `data` using numpy genfromtxt function.\n",
    "The column names are given in the variable `columns` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.533246Z",
     "start_time": "2019-03-20T08:29:27.531409Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['income', 'age', 'education', 'private-work', 'married', 'capital-gain', 'capital-loss', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.719934Z",
     "start_time": "2019-03-20T08:29:27.534318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt(\"05-dataset.tsv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will implement a neural network using only numpy functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks for a neural network\n",
    "\n",
    "Neural network libraries like PyTorch and TensorFlow seal different functionalities into different classes.\n",
    "\n",
    "First, implement the fully connected layer which does $\\mathbf{y} = X\\mathbf{w} + \\mathbf{b}$.\n",
    "It is also called Linear layer in PyTorch or Dense layer in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.727328Z",
     "start_time": "2019-03-20T08:29:27.721568Z"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnectedLayer():\n",
    "    \"\"\"\n",
    "    This is a class skeleton provided.\n",
    "    It should perform y = Xw + b and its corresponding gradient.\n",
    "    If you never defined any classes in python before, you probably want to read other tutorials.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        This is the init function where you have all the attributes needed defined.\n",
    "        You don't have to modify any thing in this function but you should read it carefully.\n",
    "        What each represents will be explained in the next few functions.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = np.zeros((in_features, out_features))\n",
    "        self.bias = np.zeros((out_features, 1))\n",
    "        self.g_weight = np.zeros((in_features, out_features))\n",
    "        self.g_bias = np.zeros((out_features, 1))\n",
    "        self.input = None\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Currently, the weight and bias of this layer is initilized to zero, which is terrible.\n",
    "        You want to re-initilize the weight with standard normal distribution \n",
    "        and the bias with uniform distribution defined on range 0 to 1.\n",
    "        Or you can try different initilization methods.\n",
    "        After you finish, comment out raise NotImplementedError.\n",
    "        No return value is needed.\n",
    "        \"\"\"\n",
    "        self.weight = np.random.rand(0, 1, self.weight.shape)\n",
    "        self.bias = np.random.uniform(0, 1, self.bias.shape)\n",
    "        return None\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Take the output of last layer as X and return the result.\n",
    "        Don't forget to save the input X to self.input. You will need the input for gradient calculation.\n",
    "        After you finish, comment out raise NotImplementedError.\n",
    "        If you are new to python/numpy, you probably want to figure out what is broadcasting \n",
    "        (see http://cs231n.github.io/python-numpy-tutorial/#numpy-broadcasting).\n",
    "        \"\"\"\n",
    "        self.input = X\n",
    "        #     (out x k) @ (k x in)    (out x 1)\n",
    "        out = (self.weight@X.T) + self.bias\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def backward_pass(self, g_next_layer):\n",
    "        \"\"\"\n",
    "        g_next_layer is the gradient passed from next layer (the layer after current layer in forward pass).\n",
    "        You need to calculate 3 things.\n",
    "        First, the gradient with respect to bias, self.g_bias.\n",
    "        Second, the gradient with respect to weights, self.g_weight.\n",
    "        Third, the gradient with respect to last layer (the layer formed by the current weight and bias), g_last_layer.\n",
    "        Save the gradient with respect to bias and the gradient with respect to weight.\n",
    "        Return the gradient with respect to last layer.\n",
    "        \"\"\"\n",
    "        self.g_weight = self.input\n",
    "        self.g_bias = 1\n",
    "\n",
    "        g_last_layer = self.weight\n",
    "        return g_last_layer\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weight and bias use the gradient that you just calculated.\n",
    "        No return is needed.\n",
    "        \"\"\"\n",
    "        self.weight = self.weight + self.g_weight\n",
    "        self.bias = self.bias + self.g_bias\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "\n",
    "Why is initialising weights and biases to zero terrible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement sigmoid function and sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.745508Z",
     "start_time": "2019-03-20T08:29:27.738898Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    \"\"\"\n",
    "    Make sure that you function works with X being matrix.\n",
    "    Use functions in numpy instead of functions in math.\n",
    "    \"\"\"\n",
    "    return 1(1-np.exp((-1)*X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.753060Z",
     "start_time": "2019-03-20T08:29:27.746788Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This is the init function where you have all the attributes needed defined.\n",
    "        You don't have to modify any thing in this function but you should read it carefully.\n",
    "        \"\"\"\n",
    "        self.input = None\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Apply sigmoid function to input and save the input for later.\n",
    "        \"\"\"        \n",
    "        out = sigmoid(X)\n",
    "        return out\n",
    "\n",
    "    def backward_pass(self, g_next_layer):\n",
    "        \"\"\"\n",
    "        Calculate the gradient with respect to the input.\n",
    "        g_next_layer is the gradient passed from next layer (the layer after current layer in forward pass).\n",
    "        Return the gradient with respect to the output of the last layer.\n",
    "        \"\"\"\n",
    "        g_last_layer = sigmoid(self.input) * (1 - sigmoid(self.input))\n",
    "\n",
    "        return g_last_layer\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        There is no parameter to update for this layer, but we still define this function to maintain a uniform interface.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "Why do we need the activation function to be non-linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement binary cross entropy loss and yes, this is the same loss that you used in logistic regression.\n",
    "Binary cross entropy loss can only deal with two classes. For more than two classes you need softmax function and cross entropy, but let's not worry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.771373Z",
     "start_time": "2019-03-20T08:29:27.764962Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This is the init function where you have all the attributes needed defined.\n",
    "        You don't have to modify any thing in this function but you should read it carefully.\n",
    "        \"\"\"\n",
    "        self.input_y = None\n",
    "        self.input_t = None\n",
    "        self.input_N = None\n",
    "\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        \"\"\"\n",
    "        y: batch_size * 1  0 <= y <= 1, the predictions\n",
    "        t: batch_size * 1 , the targets\n",
    "        (make sure y and t have the same shape. (N,) and (N,1) are different!)\n",
    "        \n",
    "        Save the input y, t and batch size N and calculate the loss.\n",
    "        Return the mean of the loss (a scalar).\n",
    "        \"\"\"\n",
    "        N = y.shape[0]\n",
    "        self.input_N = N\n",
    "        self.input_y = y\n",
    "        t = t.reshape((N,))\n",
    "        self.input_t = t\n",
    "        \n",
    "        ones = np.ones((N,))\n",
    "        loss = -( t@p.log(y) + (ones-t)@np.log(ones-y) )\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, g_next_layer = 1):\n",
    "        \"\"\"\n",
    "        Nomrally, loss layer is the last layer in a neural network. Thus, we set the g_next_layer to 1.\n",
    "        Calculate the loss with respect to the input y.\n",
    "        \"\"\"\n",
    "        t = self.input_t\n",
    "        y = self.input_y\n",
    "        g_last_layer = -(t/y) + (1-t)/(1-y)\n",
    "        return g_last_layer\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        There is no parameter to update for this layer, but we still define this function to maintain a uniform interface.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a neural network with one hidden layer to solve this classification problem. \n",
    "\n",
    "### Question 3\n",
    "How many input units would there be? How many output units?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put them together\n",
    "\n",
    "Put the bulding block together to form a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.786924Z",
     "start_time": "2019-03-20T08:29:27.780084Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Since our simple nerual network acts sequentially, \n",
    "        we can put all layers in a list for convenient traversal\n",
    "        Initialize all layers that you need (two fully connected layers, two sigmoid layers).\n",
    "        Append them to the list in the correct order.\n",
    "        Don't forget to initilize the weights for fully connected layers. Choose a sensible hidden layer size.\n",
    "        \"\"\"\n",
    "        self.sequential = []\n",
    "        \n",
    "    def __init__(self, neurnums):\n",
    "        \"\"\"\n",
    "        X is input (n*d)\n",
    "        t is examples\n",
    "        neurnums is list of number of neurons in each layer\n",
    "        \"\"\"\n",
    "        self.neurnums = neurnums\n",
    "        self.sequential = []\n",
    "        \n",
    "        for i in range(len(neurnums)-1):\n",
    "            self.sequential.append( FullyConnectedLayer(neurnums[i], neurnums[i+1]) )\n",
    "            self.sequential.append( Sigmoid() )\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        X = X\n",
    "        for l in self.sequential:\n",
    "            X = l.forward_pass(X)\n",
    "        return X\n",
    "\n",
    "    def backward_pass(self, grad):\n",
    "        grad = grad\n",
    "        for l in reversed(self.sequential):\n",
    "            grad = l.backward_pass(grad)\n",
    "            \n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        for l in self.sequential:\n",
    "            l.update(learning_rate)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.794516Z",
     "start_time": "2019-03-20T08:29:27.788037Z"
    }
   },
   "outputs": [],
   "source": [
    "# define network architecture\n",
    "neurnums = [data.shape[1], 5, 7, 1] #NN with 2 hidden layers each with 5 and 7 neurons respectively\n",
    "\n",
    "# separate train and test data\n",
    "train_ratio = 0.7\n",
    "data_split = int(data.shape[0]*0.7)\n",
    "train_data = data[:data_split,:]\n",
    "test_data = data[data_split:,:]\n",
    "\n",
    "train_feat = train_data[:,1:]\n",
    "train_target = train_data[:,0]\n",
    "\n",
    "test_feat = test_data[:,1:]\n",
    "test_target = test_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.802105Z",
     "start_time": "2019-03-20T08:29:27.795497Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = Network(neurnums)\n",
    "bce = BinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already set up a neural network.\n",
    "In the backward_pass function of each layer, you calculated the gradient of the input and the gradient of the weight.\n",
    "Now, get a pen and a paper and try to replace g_next_layer with the backward_pass function of the next layer, all the way back to binary cross entropy loss. Don't change any code, just write it down.\n",
    "\n",
    "Compare the results with the text book.\n",
    "I hope you can see how this layer structure naturally implements the chain rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network\n",
    "\n",
    "Split your data in half randomly, into a test set and a training set. Train the neural network on your training set. You may want to google train_test_split and accuracy_score of sklearn.\n",
    "If you get any memory error, make sure that all vectors in you calculate have shape (D, 1). (D, 1) and (D,) are different.\n",
    "Don't forget to normalize each feature to get mean 0 and variance 1.\n",
    "\n",
    "Plot your training accuracy curve againt testing accuracy curve.\n",
    "Plot your training loss curve againt testing loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:29:27.807289Z",
     "start_time": "2019-03-20T08:29:27.803080Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generate your training and testing set here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \"\"\"\n",
    "    This is the main training loop.\n",
    "    You need to first run a forward pass, get the predicted probability, \n",
    "    put it into the loss function, calculate the loss and do a back pass.\n",
    "    Then update the network.\n",
    "    Calculate the test loss and the current accuracy on both the train and test sets.\n",
    "    Save these to a list for plotting later.\n",
    "    Experiment to find a good learning rate.\n",
    "    \"\"\"\n",
    "    train_acc = net.forward_pass()\n",
    "    \n",
    "    train_loss = None\n",
    "\n",
    "    grad = None\n",
    "\n",
    "    \"\"\"\n",
    "    Calcualte the accuracy and loss on the testing set.\n",
    "    \"\"\"\n",
    "    \n",
    "    test_acc = None\n",
    "    \n",
    "    test_loss = None\n",
    "\n",
    "    print(\"iteration %d: train_acc %f, train_loss %f, test_acc %f, test_loss %f\" %(i+1, train_acc, train_loss, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:30:13.340163Z",
     "start_time": "2019-03-20T08:29:27.808303Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T08:30:13.556594Z",
     "start_time": "2019-03-20T08:30:13.464025Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an super cool website:https://playground.tensorflow.org/.\n",
    "Have fun in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
